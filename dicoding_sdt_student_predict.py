# -*- coding: utf-8 -*-
"""Copy of Dicoding SDT - Credit Score Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dduovbO-FIAThdYa5UIG7T3ioSUeTS1Z

# Proyek Akhir: Menyelesaikan Permasalahan Jaya Jaya Institut

- Nama: Selly Rizkiyah
- Email: sellyrizkiyah01@gmail.com
- Id Dicoding: selly_rk

# Persiapan

## Menyiapkan library yang dibutuhkan
"""

import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import joblib

"""## Menyiapkan data yang digunakna"""

url = "https://raw.githubusercontent.com/dicodingacademy/dicoding_dataset/f4a7541bc3dfca0012e20778e135c03b3e76dc67/students_performance/data.csv"

df = pd.read_csv(url, sep=';')
df

"""# Data Understanding"""

df.info()

print(df.columns)

"""Dapat dilihat kolom-kolom yang ada di data

Dataini mencakup informasi yang diketahui pada saat pendaftaran mahasiswa, terdiri dari 4424 baris dan 36 kolom, yaitu:
- 'Marital_status': status kawin,
- 'Application_mode': tipe saat mendaftar,
- 'Application_order': urutan daftar,
- 'Course': program studi,
- 'Daytime_evening_attendance': kehadiran kelas malam atau siang,
- 'Previous_qualification': pendidikan sebelumnya,
- 'Previous_qualification_grade': nilai pendidikan sebelumnya,
- 'Nacionality': kebangsaan,
- 'Mothers_qualification': pendidikan ibu,
- 'Fathers_qualification': pendidikan ayah,
- 'Mothers_occupation': pekerjaan ibu,
- 'Fathers_occupation': pekerjaan ayah,
- 'Admission_grade': kelas yang masuk,
- 'Displaced': status mengungsi,
- 'Educational_special_needs': status kebutuhan pendidikan spesial,
- 'Debtor': status hutang,
- 'Tuition_fees_up_to_date': status biaya pendidikan terkini,
- 'Gender': jenis kelamin,
- 'Scholarship_holder': status pemegang beasiswa,
- 'Age_at_enrollment': usia saat mendaftar,
- 'International': status mahasiswa internasional,
- 'Curricular_units_1st_sem_credited': jumlah sks semester 1
- 'Curricular_units_1st_sem_enrolled': jumlah sks saat mendaftar di semester 1
- 'Curricular_units_1st_sem_evaluations': jumlah sks yang diikuti saat ujian di semester 1
- 'Curricular_units_1st_sem_approved': jumlah sks yang lulus di semester 1
- 'Curricular_units_1st_sem_grade': jumlah sks yang mendapat nilai di semester 1,
- 'Curricular_units_1st_sem_without_evaluations': jumlah sks yang tidak ikut ujian di semester 1,
- 'Curricular_units_2nd_sem_credited': jumlah sks semester 2,
- 'Curricular_units_2nd_sem_enrolled': jumlah sks saat mendaftar di semester 2,
- 'Curricular_units_2nd_sem_evaluations': jumlah sks yang diikuti saat ujian di semester 2,
- 'Curricular_units_2nd_sem_approved': jumlah sks yang lulus di semester 2
- 'Curricular_units_2nd_sem_grade': jumlah sks yang mendapat nilai di semester 2,
- 'Curricular_units_2nd_sem_without_evaluations', jumlah sks yang mendapat nilai di semester 2,
- 'Unemployment_rate': tingkat pengangguran,
- 'Inflation_rate': tingkat inflasi,
- 'GDP': PDB,
- 'Status': status mahasiswa.

## Data Cleaning
"""

df.isnull().sum()

"""Dari hasil, terlihat jika data bersih dari nilai kosong.

### Pembersihan Data Numerik
"""

df.info()

"""Seperti pada hasil di atas, tipe data di beberapa fitur belum sesuai. Misalnya seperti infomrasi yang dilihat pada metadata, beberapa fitur kategorikal non biner, menggunakan angka kode sebagai kategoriknya, bukan angka ordinal. Misalnya: pada Course 33 - Biofuel Production Technologies dan 171 - Animation and Multimedia Design. Nah, untuk menghindari kesalahan diartikan sebagai angka, maka perlu dilakukan pengubahan tipe data menjadi string pada beberapa fitur seperti ini"""

df['Course'] = df['Course'].astype(str)
df['Marital_status'] = df['Marital_status'].astype(str)
df['Application_mode'] = df['Application_mode'].astype(str)
df['Previous_qualification'] = df['Previous_qualification'].astype(str)
df['Nacionality'] = df['Nacionality'].astype(str)
df['Mothers_qualification'] = df['Mothers_qualification'].astype(str)
df['Fathers_qualification'] = df['Fathers_qualification'].astype(str)
df['Mothers_occupation'] = df['Mothers_occupation'].astype(str)
df['Fathers_occupation'] = df['Fathers_occupation'].astype(str)

df.info()

"""Terlihat jika tipe data berhasil diubah. Selanjutnya, dilakukan pemeriksaan statistika deskriptif pada data"""

df.describe(include='all')

"""Dari hasil, nilai minimum negatif dan range sangat kecil pda GDP min -4.06, max 3.51, dimaana ini kemungkinan besar dalah data tahunan yang disisipkan ke seluruh siswa, sehingga, variasinya tidak berasal dari individu. Sama halnya seperti Unemployment_rate	Inflation_rate, maka ketiga variabel ini akan dihapus

"""

df.drop(columns=["Unemployment_rate", "Inflation_rate", "GDP"], inplace=True)
df

"""Setelah menghilangkan beberapa feature tersebut, dilanjutkan proses pembersihan data. Pada proyek ini, dimulai dengan membersihkan feature numerik"""

numerical_columns = [
    'Application_order',
    'Previous_qualification_grade',
    'Admission_grade',
    'Age_at_enrollment',
    'Curricular_units_1st_sem_credited',
    'Curricular_units_1st_sem_enrolled',
    'Curricular_units_1st_sem_evaluations',
    'Curricular_units_1st_sem_approved',
    'Curricular_units_1st_sem_grade',
    'Curricular_units_1st_sem_without_evaluations',
    'Curricular_units_2nd_sem_credited',
    'Curricular_units_2nd_sem_enrolled',
    'Curricular_units_2nd_sem_evaluations',
    'Curricular_units_2nd_sem_approved',
    'Curricular_units_2nd_sem_grade',
    'Curricular_units_2nd_sem_without_evaluations']

df[numerical_columns].describe()

"""Dari hasil di atas, dapat dilihat
- jika nilai max pada Age_at_enrollment adalah 70, dimana ini cukup tinggi dari jarak Q3nya di 25
- nilai max Curricular_units_1st_sem_enrolled sebesar 26 juga cukup tinggi dari rata-ratanya di 6, begitu juga di Curricular_units_1st_sem_enrolled yang bernilai 23
- nilai max di Curricular_units_1st_sem_without_evaluations dan Curricular_units_2st_sem_without_evaluations yang sama-sama sebesar 12 juga bernilai cukup jauh dari rata-ratanya yang sama di 0

Untuk mlihat info lebih lanjut pada data di atas, dilakukan pemanggilan seperti ini
"""

df[df["Age_at_enrollment"] == 70]

df[df["Curricular_units_1st_sem_enrolled"] == 26]

df[df["Curricular_units_1st_sem_without_evaluations"] == 12]

df[df["Curricular_units_2nd_sem_without_evaluations"] == 12]

"""Nilai-nilai outlier di atas perlu diatasi karena akan mengganggu di tahap pemodelan jika dibiarkan, walaupun jumlahnya sedikit. Untuk memperbaikinya, mengubah sebagian besar invalid value sebagai missing value atau nilai nol dengan pemberian aturan batasan dengan lambda dan np.nan


Misalnya, Age_at_enrollment dibatasi maksimal usia 40, sementara usia di atasnya akan di nol kan. Curricular_units_sem_enrolled dibatasi menjadi 20 dan evaluationnya 8. serta grade di angka maskimal 20
"""

df["Age_at_enrollment"] = df["Age_at_enrollment"].apply(lambda x: np.nan if x < 17 or x > 40 else x)
df["Curricular_units_1st_sem_enrolled"] = df["Curricular_units_1st_sem_enrolled"].apply(lambda x: np.nan if x > 20 else x)
df["Curricular_units_2nd_sem_enrolled"] = df["Curricular_units_2nd_sem_enrolled"].apply(lambda x: np.nan if x > 20 else x)
df["Curricular_units_1st_sem_without_evaluations"] = df["Curricular_units_1st_sem_without_evaluations"].apply(lambda x: np.nan if x > 8 else x)
df["Curricular_units_2nd_sem_without_evaluations"] = df["Curricular_units_2nd_sem_without_evaluations"].apply(lambda x: np.nan if x > 8 else x)
df["Curricular_units_1st_sem_grade"] = df["Curricular_units_1st_sem_grade"].apply(lambda x: np.nan if x > 20 else x)
df["Curricular_units_2nd_sem_grade"] = df["Curricular_units_2nd_sem_grade"].apply(lambda x: np.nan if x > 20 else x)

df[numerical_columns].describe(include="all")

"""Lalu mengecek missing value yang terdapat dalam feature numerik."""

numerical_columns = [
    'Application_order',
    'Previous_qualification_grade',
    'Admission_grade',
    'Age_at_enrollment',
    'Curricular_units_1st_sem_credited',
    'Curricular_units_1st_sem_enrolled',
    'Curricular_units_1st_sem_evaluations',
    'Curricular_units_1st_sem_approved',
    'Curricular_units_1st_sem_grade',
    'Curricular_units_1st_sem_without_evaluations',
    'Curricular_units_2nd_sem_credited',
    'Curricular_units_2nd_sem_enrolled',
    'Curricular_units_2nd_sem_evaluations',
    'Curricular_units_2nd_sem_approved',
    'Curricular_units_2nd_sem_grade',
    'Curricular_units_2nd_sem_without_evaluations']

df[numerical_columns].isnull().sum()

"""Untuk membersihkan missing value tersebut, dilakukan pengisian nilai missing value dengan nilai median data"""

def fill_numerical_missing_value(column, df):
    df = df.copy()
    fill_value = df[column].median()
    df[column] = df[column].fillna(fill_value)
    return df[column]

missing_numerical_columns = [
    'Age_at_enrollment',
    'Curricular_units_1st_sem_enrolled',
    'Curricular_units_1st_sem_without_evaluations',
    'Curricular_units_2nd_sem_enrolled',
    'Curricular_units_2nd_sem_without_evaluations'
]

for col in missing_numerical_columns:
    df[col] = fill_numerical_missing_value(column=col, df=df)

"""Kode di atas akan membersihkan missing value yang terdapat dalam fitur numerik tahap sebelumnya."""

numerical_columns = [
    'Application_order',
    'Previous_qualification_grade',
    'Admission_grade',
    'Age_at_enrollment',
    'Curricular_units_1st_sem_credited',
    'Curricular_units_1st_sem_enrolled',
    'Curricular_units_1st_sem_evaluations',
    'Curricular_units_1st_sem_approved',
    'Curricular_units_1st_sem_grade',
    'Curricular_units_1st_sem_without_evaluations',
    'Curricular_units_2nd_sem_credited',
    'Curricular_units_2nd_sem_enrolled',
    'Curricular_units_2nd_sem_evaluations',
    'Curricular_units_2nd_sem_approved',
    'Curricular_units_2nd_sem_grade',
    'Curricular_units_2nd_sem_without_evaluations']

df[numerical_columns].describe()

"""Dari hasil deskripsi baru di atas, nilai max nya tampak jauh lebih normal dengan meannya"""

df.info()

"""### Pembersihan Data Kategorikal"""

categorical_columns = [
    'Marital_status',
    'Application_mode',
    'Course',
    'Daytime_evening_attendance',
    'Previous_qualification',
    'Nacionality',
    'Mothers_qualification',
    'Fathers_qualification',
    'Mothers_occupation',
    'Fathers_occupation',
    'Displaced',
    'Educational_special_needs',
    'Debtor',
    'Tuition_fees_up_to_date',
    'Gender',
    'Scholarship_holder',
    'International'
]

for col in categorical_columns:
    print(df[col].value_counts())

"""Jika melihat hasil di atas, banyak data kategorik yang hanya memiliki 1 nilai di beberpaa kategorinya, untuk itu dilakukan inisialisasi penggantian dengan "Others" untuk menggabungkan bberapa data kategorik yang memiliki nilai sangat kecil, untuk menjadi satu kategori dengan nama "Others" atau Lainnya."""

def replace_rare_categories(df, column, threshold=10):
    value_counts = df[column].value_counts()
    rare_values = value_counts[value_counts < threshold].index
    df[column] = df[column].apply(lambda x: 'Others' if x in rare_values else x)
    return df

categorical_columns = [
    'Marital_status',
    'Application_mode',
    'Course',
    'Previous_qualification',
    'Nacionality',
    'Mothers_qualification',
    'Fathers_qualification',
    'Mothers_occupation',
    'Fathers_occupation',
]

for col in categorical_columns:
    df = replace_rare_categories(df, col, threshold=10)

"""Pada kode di atas, threshold diatur 10, artinya Others akan menggantikan nilai satu atau lebih kategori yang memiliki nilai data < 10."""

categorical_columns = [
    'Marital_status',
    'Application_mode',
    'Course',
    'Daytime_evening_attendance',
    'Previous_qualification',
    'Nacionality',
    'Mothers_qualification',
    'Fathers_qualification',
    'Mothers_occupation',
    'Fathers_occupation',
    'Displaced',
    'Educational_special_needs',
    'Debtor',
    'Tuition_fees_up_to_date',
    'Gender',
    'Scholarship_holder',
    'International'
]

for col in categorical_columns:
    print(df[col].value_counts())

"""Nilai Others berhasil dibuat di beberapa fitur, sehingga nilai tampak lebih ringkas"""

df[categorical_columns].isnull().sum()

"""Berdasarkan hasil tersebut, data kategorik terlihat tidak memiliki missing values

## Data Aggregasi

Karena pada data seperti Curricular_units_1st_sem_enrolled dan Curricular_units_2st_sem_enrolled, serta Curricular_units_1st_sem_approved dan Curricular_units_2st_sem_approved dan sejenisnya hampir mirip, maka kedua data ini digabungkan dan dibuat nama baru

Keempat fitur yang hampir sama digabung, sementara ftur mengandung grade dihitung dengan rata-rata sebelum digabung
"""

df_aggr = df.copy()

df_aggr['Total_enrolled'] = df_aggr['Curricular_units_1st_sem_enrolled'] + df_aggr['Curricular_units_2nd_sem_enrolled']
df_aggr['Total_approved'] = df_aggr['Curricular_units_1st_sem_approved'] + df_aggr['Curricular_units_2nd_sem_approved']
df_aggr['Total_evaluations'] = df_aggr['Curricular_units_1st_sem_evaluations'] + df_aggr['Curricular_units_2nd_sem_evaluations']
df_aggr['Total_without_evaluations'] = df_aggr['Curricular_units_1st_sem_without_evaluations'] + df_aggr['Curricular_units_2nd_sem_without_evaluations']

df_aggr['Avg_grade'] = (
    (df_aggr['Curricular_units_1st_sem_grade'] + df_aggr['Curricular_units_2nd_sem_grade']) / 2
)

cols_to_drop = [col for col in df_aggr.columns if 'Curricular_units_1st_sem' in col or 'Curricular_units_2nd_sem' in col]
df_aggr.drop(columns=cols_to_drop, inplace=True)

df_aggr.info()

"""Fitur baru berhasil terbuat

## EDA
"""

cleaned_df = df_aggr.copy()

sns.countplot(data=cleaned_df, x="Status")
plt.show()

cleaned_df.Status.value_counts()

"""Pada visualisasi data di atas, terlihat bahwa data didominasi oleh kategori Graduate. Sementara Enrolled paling sedikit. Pada konteks ini, imbalanced data tetap dibiarkan krena menjaga informasi yang terkandung dalam data

### Statistik deskriptif data
"""

cleaned_df.info()

cleaned_df.describe(include="all")

"""Berdasarkan hasil tersebut, dapat dilihat beberapa gambaran parameter statistik dari dataset yang dimiliki. Tahap berikutnya adalah membuat visualisasi untuk mempermudah dalam memahami pola dan insight yang terdapat dalam data.

### Visualisasi distribusi data numerik
"""

numerical_columns = [
    'Application_order',
    'Previous_qualification_grade',
    'Admission_grade',
    'Age_at_enrollment',
    'Total_enrolled',
    'Total_approved',
    'Total_evaluations',
    'Total_without_evaluations',
    'Avg_grade']

def numerical_dis_plot(features, df, segment_feature=None, showfliers=True):
    fig, ax = plt.subplots(len(features), 1,figsize=(15,30))
    for i, feature in enumerate(features):
        if segment_feature:
            sns.boxplot(y=segment_feature, x=feature, data=df, ax=ax[i], showfliers=showfliers)
            ax[i].set_ylabel(None)
        else:
            sns.boxplot(x=feature, data=df, ax=ax[i], showfliers=showfliers)
    plt.tight_layout()
    plt.show()

numerical_dis_plot(
    features=numerical_columns,
    df=cleaned_df
)

"""Pada visualisasi data di atas terlihat beberapa feature yang memiliki outlier seperti pada feature Total_without_Evaluation. Hal ini terjadi karena nilainya terlalu kecil. Namun, pada beberapa outlier lainnya, nilai dapat dianggap valid dan diabaikan, karena terkadang beerapa kasus tertentu membuat data menjadi memiliki beberpaa outlier.

### Visualisasi distribusi data kategorik

Pertama kita coba membuat visualisasi data untuk feature kategorik untuk melihat gambaran jumlah data untuk setiap kategori. Berikut merupakan contoh kode untuk melakukannya.
"""

categorical_columns = [
    'Marital_status',
    'Application_mode',
    'Course',
    'Daytime_evening_attendance',
    'Previous_qualification',
    'Nacionality',
    'Mothers_qualification',
    'Fathers_qualification',
    'Mothers_occupation',
    'Fathers_occupation',
    'Displaced',
    'Educational_special_needs',
    'Debtor',
    'Tuition_fees_up_to_date',
    'Gender',
    'Scholarship_holder',
    'International'
]

fig, ax = plt.subplots(len(categorical_columns), 1,figsize=(10,24))
for i, feature in enumerate(categorical_columns):
  sns.countplot(data=cleaned_df, y=feature, ax=ax[i])
plt.show()

"""Berdasarkan visualisasi data di atas, terlihat beberapa kategori yang mendominasi dalam suatu feature. Fenomena ini sering disebut imbalance data yang berpotensi mengakibatkan terjadinya bias.

### Visualisasi distribusi numerik dengan target label
"""

numerical_dis_plot(
    features=numerical_columns,
    df=cleaned_df,
    segment_feature="Status")

"""Pada visualisasi data di atas terlihat beberapa pola.

- Siswa yang memiliki grade Previous_qualification_grade, cenderung banyak yang lulus, ini juga sejalan dengan nilai kehadiran kelas (Admission_grade). Semakin sering siswa menghadiri kelas, semakin baik pula potensi kelulusannya dan semakin jauh juga dari potensi dropout.
- Siswa yang memiliki Total_enrolled atau kelas terdaftar yang banyak, juga semakin banyak yang lulus. Juga sejalan dengan Total_approved atau total sks yang lulus dan Avg_grade atau rata-rata nilai yang tinggi, cenderung semakin banyak yang lulus. Ini berarti siswa yang mendaftar banyak kelas, lulus di kelas-kelas tsb dan mendapat nilai yang baik, terhindar dari dropout
- Siswa yang Age_at_Enrollment nya di atas 25, cenderung banyak yang dropout dan yang memiliki Total_approved sedikit serta avg_grade nya rendah, juga banyak yang dropout. Ini berarti siswa yang usianya lebih matang akan rawan dropout. dan siswa yang memiliki kelas kelulusan sedikit serta nilai yang rendah juga cenderung terkna dropout

### Visualisasi distribusi kategorik dengan target label

#### Program studi teratas berdasarkan target label

Karena awalnya, Course memiliki kategorik angka acak bukan bentuk ordinal, maka perlu dilakukan mapping untuk melihat visualisasinya
"""

cleaned_df['Course'] = cleaned_df['Course'].astype(int)

course_map = {
    33: "Biofuel Production Tech",
    171: "Animation & Multimedia",
    8014: "Social Service (Evening)",
    9003: "Agronomy",
    9070: "Communication Design",
    9085: "Veterinary Nursing",
    9119: "Informatics Engineering",
    9130: "Equinculture",
    9147: "Management",
    9238: "Social Service",
    9254: "Tourism",
    9500: "Nursing",
    9556: "Oral Hygiene",
    9670: "Advertising & Marketing",
    9773: "Journalism & Communication",
    9853: "Basic Education",
    9991: "Management (Evening)"
}

cleaned_df['Course_Name'] = cleaned_df['Course'].map(course_map)

"""Setelah mapping, dibuat fungsi yang hanya mengambil 15 kategori teratas program studi"""

def sorted_countplot(cleaned_df, col, target=None, top_n=15):
    top_values = cleaned_df[col].value_counts().nlargest(top_n).index
    df_top = cleaned_df[cleaned_df[col].isin(top_values)]

    plt.figure(figsize=(10, 6))
    if target:
        sns.countplot(data=df_top, y=col, hue=target, order=top_values)
        plt.legend(title=target, bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        sns.countplot(data=df_top, y=col, order=top_values)

    plt.title(f"Top {top_n} kategori teratas: {col}")
    plt.tight_layout()
    plt.show()

sorted_countplot(cleaned_df, 'Course_Name', target='Status', top_n=15)

"""Dari hasil, dapat dilihat jika Nursing memiliki banyak siswa yang berhasil graduate, begitu juga pada Social Servis dan Journalism. Ini berbeda dengan Management yang terlihat lebih banyak yang dropout daripada graduate, sama seperti Informatics Engeineering. Secara kseluruhan, datanya terlihat cukup acak dan bermacam-macam di program studi yang ada

#### Marital status teratas berdasarkan target label

Untuk melihat apakah status kawin pengaruh terhadap dropout, dilakukan visualisasi, dengan mapping dari nilai kategorikal yang ada
"""

marital_status_map = {
    "1": "Single",
    "2": "Married",
    "4": "Divorced",
    "5": "Facto union",
    "Others": "Others"
}

cleaned_df['Marital_status_Label'] = cleaned_df['Marital_status'].map(marital_status_map)

marital_status_counts = cleaned_df['Marital_status_Label'].value_counts().nlargest(3)

plt.figure(figsize=(8, 8))
plt.pie(
    marital_status_counts.values,
    labels=marital_status_counts.index,
    autopct='%1.1f%%',
    startangle=45
)
plt.title('Top 3 Marital Status Distribution')
plt.axis('equal')
plt.show()

"""Dapat dilihat, jika Single atau belum menikah mendominasi data sebesar 89.3%, sementara sisanya disusul married. Ini berarti rata-rata siswa belum banyak yang menikah

#### Kategorik lainnya berdasarkan target label
"""

def categorical_plot(features, df, segment_feature=None):
    fig, ax = plt.subplots(len(features), 1,figsize=(10,20))
    for i, feature in enumerate(features):
        if segment_feature:
            sns.countplot(data=df, y=segment_feature, hue=feature, ax=ax[i])
        else:
            sns.countplot(data=df, x=feature, ax=ax[i])
    plt.tight_layout()
    plt.show()

categorical_plot(
    features=[
        'Daytime_evening_attendance',
        'Displaced',
        'Educational_special_needs',
        'Debtor',
        'Tuition_fees_up_to_date',
        'Gender',
        'Scholarship_holder',
        'International'],
    df=cleaned_df,
    segment_feature="Status")

"""Pada visualisasi data di atas, terdapat beberapa pola menarik speerti.
        'Daytime_evening_attendance',
        'Displaced',
        'Educational_special_needs',
        'Debtor',
        'Tuition_fees_up_to_date',
        'Gender',
        'Scholarship_holder',
        'International'],

- Siswa yang Graduate cenderung Daytime_evening_attendance hadir pada kelas siang hari, siswa yang mengungsi (displaced), Tuition_fees_up_to_date artinya masih mendapat biaya terkini dan  dan pemegang beasiswa (Scolarship_holder)
- Siswa yang Dropout cenderung Daytime_evening_attendance hadir pada kelas siang hari, bukan siswa yang mengungsi (displaced), Tuition_fees_up_to_date masih banyak yang 0 artinya masih banyak yang tidak mendapat biaya terkini tetapi masih pemegang beasiswa (Scolarship_holder)

Di sisi lain, hasil visualisasi dari feature Gender, Debtor, Educational_special_needs dan INternational tidak terdapat pola yang menarik.

### Visualisasi hubungan variabel numerik
"""

sns.pairplot(cleaned_df, hue='Status', vars=cleaned_df[numerical_columns])

"""Dari hubungan scatter plot di atas, tidak banyak pola menarik yang ada, tetapi beberapa data terlihat memiliki pola hubungan yang tinggi dengan adanya garis linear yang terbentuk, seperti Total_approved dan Total_enrolled.

### Heatmap korelasi variabel numerik
"""

plt.figure(figsize=(20,20))
sns.heatmap(cleaned_df[numerical_columns].corr(), annot=True, cmap='coolwarm', linewidth=1)
plt.show()

"""Pada visualisasi data di atas, terlihat beberapa fitur yang saling berkorelasi secara positif maupun negatif. Hal ini menandakan adanya multikolinearitas pada dataset yang dimiliki. Namun, dari hasil di atas, data seperti Total_enrolled dan Total_approved memiliki korelais yang cukup baik yaitu 75%

# Data Preprocessing

## Train-test Split

Sebelum membagi data, berdasarkan hasil-hasil analisis sebelumnya, beberapa fitur yang tidak begitu memiliki pola yang berkontribusi secara banyak, akan dihapus. Seperti berikut ini
"""

new_cleaned_df = cleaned_df.drop(columns=['Marital_status', 'Application_mode', 'Previous_qualification',
                                          'Nacionality', 'Mothers_qualification', 'Fathers_qualification',
                                          'Mothers_occupation', 'Fathers_occupation', 'Debtor', 'Gender',
                                          'International','Course', 'Course_Name', 'Marital_status_Label', 'Displaced'], axis=1)
new_cleaned_df

new_cleaned_df.info()

"""Kode di bawah akan membagi new_cleaned_df menjadi dua bagian (train_df dan test_df) dengan proporsi 80:20. Proses pembagian ini dilakukan secara acak (shuffle=True) untuk memastikan semua keadaan yang terdapat dalam dataset dapat terwakilkan dengan baik."""

train_df, test_df = train_test_split(new_cleaned_df, test_size=0.2, random_state=42, shuffle=True)
train_df.reset_index(drop=True, inplace=True)
test_df.reset_index(drop=True, inplace=True)

print(train_df.shape)
print(test_df.shape)

"""## Encoding dan Scaled

Encoding merupakan proses untuk membuat sebuah indeks dalam bentuk bilangan bulat yang mewakili kategori tertentu dalam sebuah feature kategorik, untuk pemodelan lebih lanjut.

Selain melakukan encoding terhadap feature kategorik, perlu dilakukan scaling pada feature numerik. Scaling merupakan proses pengubahan skala pada data numerik. Proses ini dilakukan agar seluruh feature numerik memiliki skala yang terstandarisasi, untuk meningkatkan proses proses pelatihan model nantinya.

Sebelum melakukan encoding dan scaling, perlu memisahkan antara feature training (X) dan target (Y).
"""

X_train = train_df.drop(columns="Status", axis=1)
y_train = train_df["Status"]

X_test = test_df.drop(columns="Status", axis=1)
y_test = test_df["Status"]

os.makedirs("model", exist_ok=True)

def scaling(features, df, df_test=None):
    if df_test is not None:
        df = df.copy()
        df_test = df_test.copy()
        for feature in features:
            scaler = MinMaxScaler()
            X = np.asanyarray(df[feature])
            X = X.reshape(-1,1)
            scaler.fit(X)
            df["{}".format(feature)] = scaler.transform(X)
            joblib.dump(scaler, "model/scaler_{}.joblib".format(feature))

            X_test = np.asanyarray(df_test[feature])
            X_test = X_test.reshape(-1,1)
            df_test["{}".format(feature)] = scaler.transform(X_test)
        return df, df_test
    else:
        df = df.copy()
        for feature in features:
            scaler = MinMaxScaler()
            X = np.asanyarray(df[feature])
            X = X.reshape(-1,1)
            scaler.fit(X)
            df["{}".format(feature)] = scaler.transform(X)
            joblib.dump(scaler, "model/scaler_{}.joblib".format(feature))
        return df

def encoding(features, df, df_test=None):
    if df_test is not None:
        df = df.copy()
        df_test = df_test.copy()
        for feature in features:
            encoder = LabelEncoder()
            encoder.fit(df[feature])
            df["{}".format(feature)] = encoder.transform(df[feature])
            joblib.dump(encoder, "model/encoder_{}.joblib".format(feature))

            df_test["{}".format(feature)] = encoder.transform(df_test[feature])
        return df, df_test
    else:
        df = df.copy()
        for feature in features:
            encoder = LabelEncoder()
            encoder.fit(df[feature])
            df["{}".format(feature)] = encoder.transform(df[feature])
            joblib.dump(encoder, "model/encoder_{}.joblib".format(feature))
        return df

"""Function scaling() digunakan untuk melakukan proses scaling dengan metode Min-max scaling (memanfaatkan MinMaxScaler() dari scikit learn), yang merupakan metode scaling menggunakan nilai minimum dan maksimum yang terdapat dalam sebuah feature dan mengubah data dalam feature tersebut ke dalam skala 0-1. Function ini juga akan menyimpan object yang digunakan untuk melakukan proses scaling pada setiap feature dalam sebuah berkas dengan format .joblib.

Pada function encoding(), dilakukan proses encoding pada seluruh feature kategorik dengan bantuan LabelEncoder() dari scikit learn.
"""

new_cleaned_df

numerical_columns = [
    'Application_order',
    'Total_evaluations',
    'Total_without_evaluations',
    'Previous_qualification_grade',
    'Admission_grade',
    'Age_at_enrollment',
    'Total_enrolled',
    'Total_approved',
    'Avg_grade'
]

categorical_columns = [
    'Daytime_evening_attendance',
    'Educational_special_needs',
    'Tuition_fees_up_to_date',
    'Scholarship_holder',
]

new_train_df, new_test_df = scaling(numerical_columns, X_train, X_test)
new_train_df, new_test_df = encoding(categorical_columns, new_train_df, new_test_df)

new_train_df.head()

new_test_df.head()

"""Pembagian berhasil dibuat dengan nilai yang sudah terstandarisasi. Selanjutnya, dilakukan juga encoder terhadap data kategorik"""

encoder = LabelEncoder()
encoder.fit(y_train)
new_y_train = encoder.transform(y_train)
joblib.dump(encoder, "model/encoder_target.joblib")

new_y_test = encoder.transform(y_test)

new_y_train

new_y_test

"""## Principal Component Analysis (PCA)

Principal Component Analysis atau PCA adalah teknik untuk mereduksi dimensi, mengekstraksi fitur, dan mentransformasi data dari “n-dimensional space” ke dalam sistem berkoordinat baru dengan dimensi m, di mana m lebih kecil dari n.

Teknik PCA ini, umumnya digunakan untuk mereduksi feature asli menjadi sejumlah kecil feature baru yang tidak berkorelasi linier. Feature baru tersebut disebut komponen utama (PC). Komponen utama ini dapat menangkap sebagian besar varians (berkorelasi dengan informasi) dalam feature asli sehingga saat teknik PCA diterapkan pada suatu data, ia hanya akan menggunakan komponen utama untuk merepresentasikan data asli.

Pada visualisasi data heatmap korelasi, terdapat beberapa fitur yang memiliki korelasi positif dan negatif, fitur yang memiliki korelasi cukup tinggi akan masuk ke PCA 1 sementara yang tidak begitu berkorelasi masuk ke PCA 2
"""

pca_numerical_columns_1 = [
    'Total_enrolled',
    'Total_approved',
    'Avg_grade',
    'Total_evaluations',
]

pca_numerical_columns_2 = [
    'Previous_qualification_grade',
    'Admission_grade',
    'Age_at_enrollment',
    'Application_order',
    'Total_without_evaluations'
]

"""Setelah mengelompokkan feature yang saling berkorelasi, perlu menyiapkan sebuah dataframe baru yang nantinya akan menampung hasil dari proses PCA."""

train_pca_df = new_train_df.copy().reset_index(drop=True)
test_pca_df = new_test_df.copy().reset_index(drop=True)

"""DIlakukan pencarian jumlah komponen utama lebih dulu pada PCA 1. Pada kode di bawah, dilakukan PCA dengan menggunakan seluruh komponen utama yang bisa terjadi (sesuai dengan jumlah feature asli)."""

pca = PCA(n_components=len(pca_numerical_columns_1), random_state=123)
pca.fit(train_pca_df[pca_numerical_columns_1])
princ_comp = pca.transform(train_pca_df[pca_numerical_columns_1])

var_exp = pca.explained_variance_ratio_.round(3)
cum_var_exp = np.cumsum(var_exp)

plt.bar(range(len(pca_numerical_columns_1)), var_exp, alpha=0.5, align='center', label='individual explained variance')
plt.step(range(len(pca_numerical_columns_1)), cum_var_exp, where='mid', label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.show()

"""Gambar di atas menunjukkan visualisasi jumlah varians untuk setiap jumlah komponen utama. Berdasarkan gambar tersebut terlihat bahwa, dengan hanya mengambil 2 komponen utama telah diperoleh lebih dari 80% varians. Ini berarti dari 4 fitur, nilai dapat terwakili oleh hanya 2 fitur. Selanjutnya, proses PCA dilakukan dengan dua komponen utama tadi lalu mengganti kelompok feature asli dengan 2 komponen utama dari proses PCA."""

pca_1 = PCA(n_components=2, random_state=123)
pca_1.fit(train_pca_df[pca_numerical_columns_1])
joblib.dump(pca_1, "model/pca_{}.joblib".format(1))
princ_comp_1 = pca_1.transform(train_pca_df[pca_numerical_columns_1])
train_pca_df[["pc1_1", "pc1_2"]] = pd.DataFrame(princ_comp_1, columns=["pc1_1", "pc1_2"])
train_pca_df.drop(columns=pca_numerical_columns_1, axis=1, inplace=True)
train_pca_df.head()

"""Kode menyimpan object PCA ini dalam berkas pca_1.joblib.

Selanjunya, dilakukan pencarian jumlah komponen utama pada PCA 2. Pada kode di bawah, dilakukan PCA dengan menggunakan seluruh komponen utama yang bisa terjadi (sesuai dengan jumlah feature asli).
"""

pca = PCA(n_components=len(pca_numerical_columns_2), random_state=123)
pca.fit(train_pca_df[pca_numerical_columns_2])
princ_comp = pca.transform(train_pca_df[pca_numerical_columns_2])

var_exp = pca.explained_variance_ratio_.round(3)
cum_var_exp = np.cumsum(var_exp)

plt.bar(range(len(pca_numerical_columns_2)), var_exp, alpha=0.5, align='center', label='individual explained variance')
plt.step(range(len(pca_numerical_columns_2)), cum_var_exp, where='mid', label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.show()

"""Berdasarkan grafik tersebut terlihat bahwa hanya dengan menggunakan 3 komponen utama telah memperoleh 90% varians. Hal ini berarti komponen utama dapat mewakili sebagian besar informasi dari seluruh feature pada kelompok ini (ada 5 feature) dan hanya menggunakan 3 komponen utama.

Kemudian dilakukan proses PCA kembali dengan jumlah komponen utama sebanyak 3. Lalu, mengganti kelompok feature tersebut dengan 3 komponen utama dari proses PCA
"""

pca_2 = PCA(n_components=3, random_state=123)
pca_2.fit(train_pca_df[pca_numerical_columns_2])
joblib.dump(pca_2, "model/pca_{}.joblib".format(2))
princ_comp_2 = pca_2.transform(train_pca_df[pca_numerical_columns_2])
train_pca_df[["pc2_1", "pc2_2", "pc2_3"]] = pd.DataFrame(princ_comp_2, columns=["pc2_1", "pc2_2", "pc2_3"])
train_pca_df.drop(columns=pca_numerical_columns_2, axis=1, inplace=True)
train_pca_df.head()

"""Sekarang data hanya memiliki 5 feature (awalnya terdapat 9 feature) yang akan digunakan untuk melatih model machine learning. Tidak lupa, dilakukan penggabungan pada kedua reduksi PCA 1 dan PCA 2."""

test_princ_comp_1 = pca_1.transform(test_pca_df[pca_numerical_columns_1])
test_pca_df[["pc1_1", "pc1_2"]] = pd.DataFrame(test_princ_comp_1, columns=["pc1_1", "pc1_2"])
test_pca_df.drop(columns=pca_numerical_columns_1, axis=1, inplace=True)

test_princ_comp_2 = pca_2.transform(test_pca_df[pca_numerical_columns_2])
test_pca_df[["pc2_1", "pc2_2", "pc2_3"]] = pd.DataFrame(test_princ_comp_2, columns=["pc2_1", "pc2_2", "pc2_3"])
test_pca_df.drop(columns=pca_numerical_columns_2, axis=1, inplace=True)
test_pca_df.head()

"""Ini artinya data yang kita miliki telah siap digunakan untuk melatih model machine learning.

# Pemodelan
"""

X_train.columns
# atau simpan fitur ini saat training:
joblib.dump(X_train.columns.tolist(), "model/features_used.joblib")

"""## Support Vector Machine (SVM)

SVM adalah algoritma klasifikasi yang mencari hyperplane terbaik yang memisahkan kelas data dengan margin maksimum. SVM bekerja sangat baik untuk dataset berdimensi tinggi dan cocok untuk klasifikasi biner maupun multikelas. Kernel-trick juga memungkinkan SVM menangani data non-linear.
"""

svm_model = SVC(kernel='linear', C=1, gamma='scale', decision_function_shape='ovr', random_state=123)

cv_scores_svm = cross_val_score(svm_model, train_pca_df, new_y_train, cv=5, scoring='f1_weighted')
print(f"SVM - F1-score rata-rata (Cross Validation): {cv_scores_svm.mean():.5f}")
print("-" * 60)

svm_model.fit(train_pca_df, new_y_train)
joblib.dump(svm_model, "model/svm_model.joblib")

"""## Decision Tree

Decision Tree membuat model prediksi berdasarkan aturan dari fitur dalam bentuk struktur pohon. Ia membagi data secara rekursif berdasarkan fitur yang paling "informatif". Algoritma ini mudah dipahami, tetapi rawan overfitting jika pohonnya terlalu dalam.
"""

tree_model = DecisionTreeClassifier(
    random_state=123,
    criterion='entropy',
    max_depth=8,
    max_features='sqrt'
)

cv_scores_tree = cross_val_score(tree_model, train_pca_df, new_y_train, cv=5, scoring='f1_weighted')
print(f"Decision Tree - F1-score rata-rata (Cross Validation): {cv_scores_tree.mean():.5f}")
print("-" * 60)

tree_model.fit(train_pca_df, new_y_train)

joblib.dump(tree_model, "model/tree_model.joblib")

"""## Random Forest

Random Forest adalah pengembangan dari Decision Tree. Setiap pohon dibangun dengan subset data dan fitur yang berbeda. Hasil prediksi diperoleh dengan voting mayoritas. Metode ini mengurangi overfitting dan meningkatkan akurasi dibandingkan single tree.
"""

rdf_model = RandomForestClassifier(
    random_state=123,
    max_depth=8,
    n_estimators=300,
    max_features='log2',
    criterion='gini')

cv_scores_rdf = cross_val_score(rdf_model, train_pca_df, new_y_train, cv=5, scoring='f1_weighted')
print(f"Random Forest - F1-score rata-rata (Cross Validation): {cv_scores_rdf.mean():.5f}")
print("-" * 60)

rdf_model.fit(train_pca_df, new_y_train)
joblib.dump(rdf_model, "model/rdf_model.joblib")

"""## XGBoost

XGBoost adalah algoritma boosting berbasis pohon yang sangat efisien dan akurat. Ia bekerja dengan membangun model bertahap, di mana setiap model baru berusaha memperbaiki kesalahan dari model sebelumnya. XGBoost juga memiliki banyak fitur regularisasi sehingga sangat unggul dalam kompetisi machine learning.
"""

xgb_model = XGBClassifier(
    objective='multi:softmax',
    num_class=3,
    max_depth=5,
    learning_rate=0.1,
    n_estimators=100,
    eval_metric='mlogloss',
    random_state=123
)

cv_scores_xgb = cross_val_score(xgb_model, train_pca_df, new_y_train, cv=5, scoring='f1_weighted')
print(f"XGBoost - F1-score rata-rata (Cross Validation): {cv_scores_xgb.mean():.5f}")
print("-" * 60)

xgb_model.fit(train_pca_df, new_y_train)
joblib.dump(xgb_model, "model/xgb_model.joblib")

"""# Evaluasi

Dibuat label class target untuk confusion matrix. confusion_matrix(): digunakan untuk menghasilkan confusion matrix dari hasil prediksi. classification_report(): digunakan untuk menghasilkan berbagai berbagai metrik untuk mengevaluasi model seperti accuracy, recall, precision, dan F1 score.
"""

class_labels = ['Dropout', 'Graduate', 'Enrolled']

"""## Support Vector Machine (SVM)"""

y_svm_pred = svm_model.predict(test_pca_df)
accuracy_svm = accuracy_score(new_y_test, y_svm_pred)
precision_svm = precision_score(new_y_test, y_svm_pred, average='weighted')
recall_svm = recall_score(new_y_test, y_svm_pred, average='weighted')
f1_svm = f1_score(new_y_test, y_svm_pred, average='weighted')

print(f"SVM - Accuracy: {accuracy_svm:.5f}")
print(f"SVM - Precision: {precision_svm:.5f}")
print(f"SVM - Recall: {recall_svm:.5f}")
print(f"SVM - F1 Score: {f1_svm:.5f}")

cm_svm = confusion_matrix(new_y_test, y_svm_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svm, annot=True, fmt="d", cmap="Blues",
            xticklabels=class_labels, yticklabels=class_labels)
plt.title("Confusion Matrix - SVM")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Dari hasil SVM, terlihat data memiliki hasil evaluasi yang cukup rendah, yaitu 47%, sehingga model ini kemungkinan kurang sesuai untuk klasifikasi tiga label target.

## Decision Tree
"""

y_tree_pred = tree_model.predict(test_pca_df)

accuracy_tree = accuracy_score(new_y_test, y_tree_pred)
precision_tree = precision_score(new_y_test, y_tree_pred, average='weighted')
recall_tree = recall_score(new_y_test, y_tree_pred, average='weighted')
f1_tree = f1_score(new_y_test, y_tree_pred, average='weighted')

print(f"Decision Tree - Accuracy: {accuracy_tree:.5f}")
print(f"Decision Tree - Precision: {precision_tree:.5f}")
print(f"Decision Tree - Recall: {recall_tree:.5f}")
print(f"Decision Tree - F1 Score: {f1_tree:.5f}")

cm_dt = confusion_matrix(new_y_test, y_tree_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, fmt="d", cmap="Blues",
            xticklabels=class_labels, yticklabels=class_labels)
plt.title("Confusion Matrix - Decision Tree")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Decision Tree terlihat memiliki hasil yang lebih baik dari SVM, DT memiliki hasil evaluasi yang cukup baik, yaitu sekitar 68%, sehingga model ini kemungkinan lebih sesuai untuk klasifikasi tiga label target.

## Random Forest
"""

y_rdf_pred = rdf_model.predict(test_pca_df)

accuracy_rdf = accuracy_score(new_y_test, y_rdf_pred)
precision_rdf = precision_score(new_y_test, y_rdf_pred, average='weighted')
recall_rdf = recall_score(new_y_test, y_rdf_pred, average='weighted')
f1_rdf = f1_score(new_y_test, y_rdf_pred, average='weighted')

print(f"Random Forest - Accuracy: {accuracy_rdf:.5f}")
print(f"Random Forest - Precision: {precision_rdf:.5f}")
print(f"Random Forest - Recall: {recall_rdf:.5f}")
print(f"Random Forest - F1 Score: {f1_rdf:.5f}")

cm_rdf = confusion_matrix(new_y_test, y_rdf_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_rdf, annot=True, fmt="d", cmap="Blues",
            xticklabels=class_labels, yticklabels=class_labels)
plt.title("Confusion Matrix - Random Forest")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Dari Random Forest terlihat evaluasi lebih meningkat lagi, yaitu sekitar 72%, sehingga model ini lebih sesuai untuk klasifikasi tiga label target. Random Forest lebih unggul dibanding Decision TRee.

## XGBoost
"""

y_xgb_pred = xgb_model.predict(test_pca_df)
accuracy_xgb = accuracy_score(new_y_test, y_xgb_pred)
precision_xgb = precision_score(new_y_test, y_xgb_pred, average='weighted')
recall_xgb = recall_score(new_y_test, y_xgb_pred, average='weighted')
f1_xgb = f1_score(new_y_test, y_xgb_pred, average='weighted')

print(f"XGBoost - Accuracy: {accuracy_xgb:.5f}")
print(f"XGBoost - Precision: {precision_xgb:.5f}")
print(f"XGBoost - Recall: {recall_xgb:.5f}")
print(f"XGBoost - F1 Score: {f1_xgb:.5f}")

cm_xgb = confusion_matrix(new_y_test, y_xgb_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb, annot=True, fmt="d", cmap="Blues",
            xticklabels=class_labels, yticklabels=class_labels)
plt.title("Confusion Matrix - XGBoost")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Dari hasil XGBoost terlihat evaluasi hampir sama seperti Random Forest, yaitu sekitar 71%, sehingga model ini cukup sesuai untuk klasifikasi tiga label target. Random Forest masih lebih unggul 1% dari XGBoost.

Sebenarnya dari ketiga model tersebut belum ada model yang memiliki performa yang sangat baik dalam menangani data. Namun, berdasarkan hasil evaluasi ini, dapat disimpulkan bahwa model dengan performa terbaik adalah Random Forest. Model inilah yang akan digunakan dalam membuat prototype sederhana untuk mengidentifikasi dropout pada mahasiswa
"""

def plot_feature_importances(feature_importances, cols):
    features = pd.DataFrame(feature_importances, columns=['coef_value']).set_index(cols)
    features = features.sort_values(by='coef_value', ascending=False)
    top_features = features

    plt.figure(figsize=(10, 6))
    sns.barplot(x='coef_value', y=features.index, data=features)
    plt.show()
    return top_features

plot_feature_importances(tree_model.feature_importances_, train_pca_df.columns)

joblib.dump(xgb_model, "model/xgb_model.joblib")

joblib.dump(rdf_model, "model/rdf_model.joblib")

cleaned_df.to_csv('cleaned_df.csv', index=False)

new_cleaned_df.to_csv('new_cleaned_df.csv', index=False)

pip freeze > requirements.txt